---
title: 'A (very) simple Kalman Filter : Application to inflation'
author: "Brendan Berthold"
date: "4/8/2020"
output:
  html_document: default
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, include = FALSE)
```

# Introduction and motivation

This document provides a short introduction to Kalman Filtering. The first section goes rapidly over the theory behind it and emphasizes the key ideas to understand the basic principle of the Kalman Filter. The second section provides an application using a very simple example. This example is not thought to be realistic but rather seems to me quite interesting to understand basically what a Kalman Filter does. Once this basic mechanism is understood, it should be easier to apply a Kalman Filter to more complex problems.

This document can be useful as an introduction because it provides a step by step explanation of the Kalman Filter and illustrates it with a simple yet interesting example applied to inflation. To do so, the application remains in one dimension in order to only focus on what really the Kalman Filter is all about. The idea is really to focus on the implementation and the basic intuition behind the filter without having to think about complex matrix optimization.


# The Kalman Filter for Economics : Definition, theory, and intuition

We typically observe economic data over time. These data, however, are likely to be noisy. The Kalman Filter is based on the assumption that there exists some sort of underlying process (a state equation) that drives the observations but which we cannot observe because of noise and surprise shocks. The Kalman Filter is a method that allows to use the observed data in order to learn about the unobservable state variable.

The Kalman Filter was first used in physics but is now widely popular in economics.

# The theory

## Notation

The general form of the Kalman filter as presented in Hamilton Chapter 13 (add ref) is given by a "measurement equation":
\begin{align}
y_t = A'x_t + H'\xi_t + w_t \label{eq1}
\end{align}
With $E(w_tw_t')=Q$

And a transition (or state) equation:
\begin{align}
\xi_t = F\xi_{t-1} + v_t \label{eq2}
\end{align}
With $E(v_tv_t')=R$.

Notation:

- $y_t$ is the vector of observed variables (i.e. the data)
- $x_t$ is a vector of deterministic components (we won't spend time on it in this document)
- $\xi_t$ is the unobserved "state" variables
- $w_t$ and $v_t$ are unobserved, mutually and serially uncorrelated noise variables
- $A, H, R, F,$ and $Q$ are non-random "system" variables matrices that may depend on unknown parameters (some of them can be retrieved using Maximum Likelihood estimations)


The general system defined by (\ref{eq1}) and (\ref{eq2}) is flexible and can accomodate a variety of representation. For instance, a standard AR(p) process fits into the general notation in the following way:

Let $y_t \sim AR(p)$, that is:
\begin{align*}
y_t = \phi_1 y_{t-1} + \phi_2y_{t-2} + ... + \phi_py_{t-p} + \epsilon_t
\end{align*}
This process can be represented as a "state-space" model in the following way:
\begin{align*}
&\xi_t = \begin{bmatrix}
y_t \\ y_{t-1} \\ \vdots \\ y_{t-p+1}
\end{bmatrix} \\
&F= \begin{bmatrix} 
\phi_1 & \phi_2 & \ldots & \phi_{p-1} & \phi_p \\
1 & 0 & \ldots & 0 & 0 \\
0 & 1 &&& 0 \\
\vdots & & \ddots & & \vdots \\
0 &&&1&0
\end{bmatrix} \\
&v_t = \begin{bmatrix}
\epsilon_t \\ 0 \\ \vdots \\ 0
\end{bmatrix}
\end{align*}
And $w_t=0, A=0,$ and $H'=\begin{bmatrix}1 & 0 & \ldots & 0\end{bmatrix}$


## Procedure and idea of the Kalman Filter

Notation:

- $y_{1:t} = \left\{y_i\right\}^t_{i=1}$
- $\xi_{t|k} = E(\xi_t|y_{1:k})$
- $P_{t|k}=Var(\xi_{t}|y_{1:k})$


In words, the Kalman filter is a recursive algorithm to construct $\xi_{t|t}$ and $P_{t|t}$ from known values in $t$, that is $y_t,x_t, \xi_{t-1|t-1}, P_{t-1|t-1}$. 

To derive the filter, we assume that both $w_t$ and $v_t$ follow iid Gaussian process, that is:
\begin{align*}
\begin{bmatrix}
w_t \\ v_t
\end{bmatrix} \sim N \left(\begin{bmatrix}0\\0\end{bmatrix}, \begin{bmatrix}R &0\\0&Q\end{bmatrix}\right)
\end{align*}
This notably implies that both $y_t$ and $\xi_t$ follow a _joint_ Normal distribution. Since errors are Gaussian, the best estimator (in the sense that it minimises the mean squared error) is given by the conditional expectation.

To find the conditional expectation of $\xi_t$ and $y_t$ (that is $\xi_{t|t}$ and $y_{t|t}$), we can use the following theorem on the conditional distribution of a multivariate normal:

Suppose that:
\begin{align*}
\begin{bmatrix}
z_1 \\ z_2
\end{bmatrix} \sim N \left(\begin{bmatrix}\mu_1\\\mu_2\end{bmatrix}, \begin{bmatrix}\Sigma_{11} &\Sigma_{12}\\\Sigma_{21}&\Sigma_{22}\end{bmatrix}\right)
\end{align*}
Then:
\begin{align*}
E(z_1|z_2) = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(z_2 - \mu_2) \\
Var(z_1|z_2) = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
\end{align*}
This theorem is the *key idea* of the Kalman Filter. In particular, the Kalman Filter is "simply" an application of it.

Defining $z_1 = \xi_t$ and $z_2 = y_t$, and recognizing that $\xi_t$ and $y_t$ are jointly Normal conditional on past values, we can write the following:
\begin{align*}
\begin{bmatrix}
\xi_t \\ y_t
\end{bmatrix} \Bigg | y_{1:t-1}\sim \mathcal{N}\left(\begin{bmatrix}
\xi_{t|t-1} \\ y_{t|t-1}
\end{bmatrix}, \begin{bmatrix}
P_{t|t-1} & \Sigma_{\xi,y|t-1} \\\Sigma_{\xi,y|t-1} & \Sigma_{yy|t-1}
\end{bmatrix}\right)
\end{align*}
Using the formula of the conditional normal:
\begin{align*}
\xi_{t|t}= \xi_{t|t-1} + \Sigma_{\xi,y|t-1}\Sigma_{yy|t-1}^{-1}(y_t-y_{t|t-1}) \\
P_{t|t} = P_{t|t-1} - \Sigma_{\xi,y|t-1}\Sigma_{yy|t-1}^{-1}\Sigma_{\xi,y|t-1}
\end{align*}



1. $\xi_{t|t-1} = F\xi_{t-1|t-1}$ 
2. $y_{t|t-1} = A'x_t + H'\xi_{t|t-1}$
3. $P_{t|t-1} = FP_{t-1|t-1}F' + Q$
4. $\Sigma_{yy|t-1} = H'P_{t|t-1}H + R \equiv h_t$
5. $\Sigma_{\xi,y|t-1}\Sigma_{yy|t-1}^{-1} = P_{t|t-1}H h_t^{-1} \equiv K_t$
6. $\eta_t = y_t-y_{t|t-1}$


Thus:
\begin{align*}
\xi_{t|t} = \xi_{t|t-1} + K_t \eta_t\\
P_{t|t} = P_{t|t-1} - K_t H'P_{t|t-1}
\end{align*}

This procedure allows us to retrieve $\xi_{t|t}$ and $P_{t|t}$ recursively (i.e. assuming $\xi_{t-1|t-1}$ and $P_{t-1|t-1}$) are known. This procedure can then be repeated for each period. We will see this in more details in the next section.


## Application

To better understand the algorithm let us consider the following (uni-dimensional) simple example. For simplicity, we assume that $\phi=1$ in the transition equation:

```{r}
setwd('~/Dropbox/My Computer/PhD Economics/Gerzensee/Econometrics/Week3/GPS1/R')
source('ASimpleKF.R')
setwd('~/Dropbox/My Computer/PhD Economics/Gerzensee/Econometrics/Week3/GPS1/R')
```

The state space model is of the form:
\begin{align*}
&y_t = \xi_t + w_t \\
&\xi_t = \xi_{t-1} + v_t
\end{align*}
Where $y_t$ is the observed inflation from period 1980 to 2019.

The first step is to approximate the value of $\sigma^2_w$ and $\sigma^2_v$, that is the variance of $w_t$ and $v_t$ respectively. To do this, we can notice the following:
\begin{align*}
\Delta y_t &= \Delta \xi_t + \Delta w_t \\
&=  v_t + \Delta w_t
\end{align*}
Using the fact that $w$ and $v$ are independent and covariance stationary, the variance is given by:
\begin{align*}
Var(\Delta y_t) = \sigma^2_{v} + 2\sigma^2_{w} \\
\end{align*}
Similarly, we can compute:
\begin{align*}
Cov(\Delta Y_t , \Delta Y_{t-1}) = - \sigma^2_{w}
\end{align*}
These two moments can be approximated using the data. In our dataset, the sample variance of the first differences is equal to `r round(var_Y_t, digits = 3)` and covariance of the first differences is equal to `r round(cov_Y_1, digits = 3)`. Using () and () as a system of two equations of two unknowns, we can retrieve:
```{r, include = F, echo = F}
sigma_sq_w <- -cov_Y_1
sigma_sq_v <- var_Y_t - 2*sigma_sq_w
```
\begin{align*}
 \sigma^2_w = - Cov\left(\Delta Y_t, \Delta Y_{t-1}\right) = `r round(sigma_sq_w, digits = 4)` \\
 \sigma^2_v = Var\left(\Delta Y_t\right) - 2\sigma^2_w = `r round(sigma_sq_v, digits = 4)`
\end{align*}

The last step is to initalize the loop. We assume $\xi_{0|0}=0$ because the process is covariance stationary, and $P_{0|0}=100$ arbitrarily.

## The algorithm filter

Since we know $E(\xi_0|t=0)=\xi_{0|0}$ $Var(\xi_t|t=0)=P_{0|0}$, we can recursively compute $\xi_{t|t}$ and $P_{t|t}$ for $t>0$ using the Kalman Filter algorithm:

1. $\xi_{t|t-1}= \xi_{t-1|t-1}$
2. $y_{t|t-1} = \xi_{t|t-1}$
3. $P_{t|t-1} = P_{t-1|t-1} + \sigma_v^2$
4. $h_t \equiv Var(y_t|t-1)  = P_{t|t-1} + \sigma^2_w$
5. $K_t = Cov(\xi_t,Y_t|t-1)\times h_t = P_{t|t-1}\times h_t^{-1}$
6. $\eta_t = y_t-y_{t|t-1}$

Using this, we get our next period KF forecast:

1. $\xi_{t|t} = \xi_{t|t-1} + K_t\times \eta_t$
2. $P_{t|t} = P_{t|t-1} - K_t\cdot Cov(\xi_t,Y_t|t-1)$

Which can be written as follows in R:

```{r, echo = TRUE, include = T}
# notation: l. means the conditional expectation on t-1 while f. means the actual forecast
phi = 1
# Initialization
f.xi <- 0
f.P <- 100

f.xi[1] <- 1
f.P[1] <- 2

# loop
l.xi <- 0
l.y <- 0
l.P <- 0
h_t <- 0
K_t <- 0
eta <- 0

y_t <- data$Y_t
yt <- arima.sim(list(order=c(1,0,0), ar=.5), n=200)
plot(yt)

#plot(y_t, type = "l")
for(i in c(2:length(y_t))){
  l.xi[i] <- phi*f.xi[i-1]
  l.y[i] <- l.xi[i]
  l.P[i] <- phi*phi*f.P[i-1] + sigma_sq_v
  h_t[i] <- l.P[i] + sigma_sq_w
  K_t[i] <- l.P[i]*(h_t[i])^(-1) 
  eta[i] <- y_t[i]-l.y[i]
  f.xi[i] <- l.xi[i] + K_t[i]*eta[i]
  f.P[i] <- l.P[i] - K_t[i]*l.P[i]
}
```
The next graph provides a graphical representation of our simple Kalman Filter. In R-code jargon, it simply plots f.xi alongside y_t over time:
```{r, include = TRUE}
require(ggplot2)
ggplot() +
  ggtitle('Kalman Filter') +
  geom_line(aes(x=c(1:length(y_t)), y=y_t, colour = "data")) +
  geom_line(aes(x=c(1:length(y_t)), y=f.xi, colour = "Kalman Filter")) +
  scale_color_manual(values = c("Kalman Filter"="blue", data="black"))
  #geom_line(aes(x=c(1:length(y_t)), y=l.xi), colour = 'red')
```


If our state system correctly reflects the process of inflation (which is very unlikely given the simplicity of the example), the Kalman Filter allows us to retrieve the "structural shocks", that is the shocks that past values cannot predict. They are simply equal to $y_t - \xi_{t|t}$ and are represented in the next plot:
```{r, include = T}
# Structural shocks
shocks <- y_t - f.xi
ggplot() + 
  ggtitle("Structural Shocks") +
  geom_line(aes(x=c(1:length(y_t)), y=shocks, colour = "Structural shocks")) +
  scale_color_manual(values = c("Structural shocks"="black"))
```

# Conclusion

Simple example. Of course neglects lots of complexity. Usually, things are in matrix forms. Also, parameters of the processes are usually retrieved by ML estimation. I may write something about it at some point if there is demand.

Cheers.


# References

- Hamilton
- Mark Watson's courses in Gerzensee

