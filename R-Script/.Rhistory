geom_line(aes(x=1:n_periods, y=yt), colour = "black")
setwd("~/Dropbox/My Computer/GitHub/KalmanFilter/R-script")
ggplot(aes(x=c(1:n_periods))) +
geom_line(aes(y=f.xi)) +
geom_line(aes(y=yt))
ggplot() +
geom_line(aes(x=c(1:n_periods), y=f.xi)) +
geom_line(aes(x=c(1:n_periods), y=yt))
f.xi
yt
---
title: 'Intuition, theory, and a simple application of the Kalman Filter'
author: "Brendan Berthold"
date: "4/8/2020"
output:
html_document: default
editor_options:
chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, include = FALSE)
```
# Introduction
This document provides an intuitive and simple introduction to Kalman Filtering.
The document is organised as follows: the first section motivates the usage of a Kalman Filter in an economic context and reviews the related theory. In particular, it focuses on the key results and ideas behind it--as we shall see, the Kalman Filter's key idea is in reality surprisingly easy to understand. The second section applies the Kalman Filter algorithm to an extremely simple (yet not trivial) state-space model characterised by an AR(1) process as the state equation. The idea behind this is to simplify as much as possible in order to focus on the key elements that will then reveal useful to understand more complex problems.
This document is mostly based on lecture Notes from Prof. Mark Watson (Princeton University) given as part of the program for beginning doctoral students in Economics in Gerzensee taught in March 2020. The lecture notes are themselves heavily based on Hamilton's seminal book.
# The Kalman Filter for Economics : Motivation, theory, and intuition
## Motivation
We typically observe economic data over time. These data, however, are likely to be noisy. The Kalman Filter is based on the assumption that there exists some sort of underlying process (a state equation) that drives the observations but which we cannot observe because of noise and surprise shocks. The Kalman Filter is a method that allows to use the observed data in order to learn about the unobservable state variable.
The Kalman Filter was first used in physics but is now widely popular in economics.
## The theory
**Notation**
The general form of the Kalman filter as presented in Hamilton Chapter 13 (add ref) is given by a "measurement equation":
\begin{align}
y_t = A'x_t + H'\xi_t + w_t \label{eq1}
\end{align}
With $E(w_tw_t')=Q$
And a transition (or state) equation:
\begin{align}
\xi_t = F\xi_{t-1} + v_t \label{eq2}
\end{align}
With $E(v_tv_t')=R$.
In words:
- $y_t$ is the vector of observed variables (i.e. the data)
- $x_t$ is a vector of deterministic components (we won't spend time on it in this document)
- $\xi_t$ is the unobserved "state" variables
- $w_t$ and $v_t$ are unobserved, mutually and serially uncorrelated noise variables (which usually follow a Normal Distribution)
- $A, H, R, F,$ and $Q$ are non-random "system" variables matrices that may depend on unknown parameters (some of them can be retrieved using Maximum Likelihood estimations)
The general system defined by (\ref{eq1}) and (\ref{eq2}) is flexible and can accomodate a variety of representation. For instance, a standard AR(p) process fits into the general notation in the following way:
Let $y_t \sim AR(p)$, that is:
\begin{align*}
y_t = \phi_1 y_{t-1} + \phi_2y_{t-2} + ... + \phi_py_{t-p} + \epsilon_t
\end{align*}
This process can be represented as a "state-space" model in the following way:
\begin{align*}
&\xi_t = \begin{bmatrix}
y_t \\ y_{t-1} \\ \vdots \\ y_{t-p+1}
\end{bmatrix} \\
&F= \begin{bmatrix}
\phi_1 & \phi_2 & \ldots & \phi_{p-1} & \phi_p \\
1 & 0 & \ldots & 0 & 0 \\
0 & 1 &&& 0 \\
\vdots & & \ddots & & \vdots \\
0 &&&1&0
\end{bmatrix} \\
&v_t = \begin{bmatrix}
\epsilon_t \\ 0 \\ \vdots \\ 0
\end{bmatrix}
\end{align*}
And $w_t=0, A=0,$ and $H'=\begin{bmatrix}1 & 0 & \ldots & 0\end{bmatrix}$
### Procedure and idea of the Kalman Filter
Notation:
- $y_{1:t} = \left\{y_i\right\}^t_{i=1}$
- $\xi_{t|k} = E(\xi_t|y_{1:k})$
- $P_{t|k}=Var(\xi_{t}|y_{1:k})$
In words, the Kalman filter is a recursive algorithm to construct $\xi_{t|t}$ and $P_{t|t}$ from known values in $t$, that is $y_t,x_t, \xi_{t-1|t-1}, P_{t-1|t-1}$.
To derive the filter, we assume that both $w_t$ and $v_t$ follow iid Gaussian process, that is:
\begin{align*}
\begin{bmatrix}
w_t \\ v_t
\end{bmatrix} \sim N \left(\begin{bmatrix}0\\0\end{bmatrix}, \begin{bmatrix}R &0\\0&Q\end{bmatrix}\right)
\end{align*}
This notably implies that both $y_t$ and $\xi_t$ follow a _joint_ Normal distribution. Since errors are Gaussian, the best estimator (in the sense that it minimises the mean squared error) is given by the conditional expectation.
To find the conditional expectation of $\xi_t$ and $y_t$ (that is $\xi_{t|t}$ and $y_{t|t}$), we can use the following theorem of the conditional distribution of a multivariate normal:
Suppose that:
\begin{align*}
\begin{bmatrix}
z_1 \\ z_2
\end{bmatrix} \sim N \left(\begin{bmatrix}\mu_1\\\mu_2\end{bmatrix}, \begin{bmatrix}\Sigma_{11} &\Sigma_{12}\\\Sigma_{21}&\Sigma_{22}\end{bmatrix}\right)
\end{align*}
Then:
\begin{align*}
E(z_1|z_2) = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(z_2 - \mu_2) \\
Var(z_1|z_2) = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
\end{align*}
The application of this theorem is the **key idea** of the Kalman Filter.
Defining $z_1 = \xi_t$ and $z_2 = y_t$, and recognizing that $\xi_t$ and $y_t$ are jointly Normal conditional on past values, we can write the following:
\begin{align*}
\begin{bmatrix}
\xi_t \\ y_t
\end{bmatrix} \Bigg | y_{1:t-1}\sim \mathcal{N}\left(\begin{bmatrix}
\xi_{t|t-1} \\ y_{t|t-1}
\end{bmatrix}, \begin{bmatrix}
P_{t|t-1} & \Sigma_{\xi,y|t-1} \\\Sigma_{\xi,y|t-1} & \Sigma_{yy|t-1}
\end{bmatrix}\right)
\end{align*}
Using the formula of the conditional normal:
\begin{align*}
\xi_{t|t}= \xi_{t|t-1} + \Sigma_{\xi,y|t-1}\Sigma_{yy|t-1}^{-1}(y_t-y_{t|t-1}) \\
P_{t|t} = P_{t|t-1} - \Sigma_{\xi,y|t-1}\Sigma_{yy|t-1}^{-1}\Sigma_{\xi,y|t-1}
\end{align*}
Assuming $\xi_{t-1|t-1}$ and $P_{t-1|t-1}$ are known:
\begin{align}
&\xi_{t|t-1} = F\xi_{t-1|t-1} \\
&y_{t|t-1} = A'x_t + H'\xi_{t|t-1} \\
&P_{t|t-1} = FP_{t-1|t-1}F' + Q \\
&\Sigma_{yy|t-1} = H'P_{t|t-1}H + R \equiv h_t \\
&\Sigma_{\xi,y|t-1}\Sigma_{yy|t-1}^{-1} = P_{t|t-1}H h_t^{-1} \equiv K_t \\
&\eta_t = y_t-y_{t|t-1}
\end{align}
Applying the theorem, we get:
\begin{align*}
&\xi_{t|t} = \xi_{t|t-1} + K_t \eta_t\\
&P_{t|t} = P_{t|t-1} - K_t H'P_{t|t-1}
\end{align*}
If the process is covariance stationary, we can initialize it by assuming $\xi_{0|0}=0$ and using a reasonable estimate of $P_{0|0}$. We can then retrieve $x_{t|t}$, and $P_{t|t}$ for all $t>0$ applying this procedure recursively.
## Application
To better understand the algorithm let us consider the following (uni-dimensional) simple example.
```{r}
setwd("~/Dropbox/My Computer/GitHub/KalmanFilter/R-script")
source("ASimpleKF.R")
```
The state space model is of the form:
\begin{align*}
&y_t = \phi \xi_t + w_t \\
&\xi_t = \xi_{t-1} + v_t
\end{align*}
For simplicity, we assume that the transition equation is known with certainty and that $\phi=$ `r phi`. Moreover, we assume that $w_t$ and $v_t$ are i.i.d and independent of one another with $\sigma^2_w=$ `r sigma_sq_w` and $\sigma^2_v=$ `r sigma_sq_v`.
Where the generated $y_t$ looks as follows:
```{r, include = T}
ggplot() +
ggtitle("Randomly Generated Data") +
geom_line(aes(x=1:n_periods, y=yt), colour = "black")
```
The last step is to initalize the loop. We assume $\xi_{0|0}=0$ because the process is covariance stationary, and $P_{0|0}=1$ arbitrarily.
## The algorithm
Since $\xi_{0|0}$ and $P_{0|0}$ are known, we can recursively compute $\xi_{t|t}$ and $P_{t|t}$ for $t>0$ using the Kalman Filter algorithm:
\begin{align}
\xi_{t|t-1}= \phi \xi_{t-1|t-1} \\
y_{t|t-1} = \xi_{t|t-1} \\
P_{t|t-1} = \phi^2P_{t-1|t-1} + \sigma_v^2 \\
h_t \equiv Var(y_t|t-1)  = P_{t|t-1} + \sigma^2_w \\
K_t = Cov(\xi_t,Y_t|t-1)\times h_t = P_{t|t-1}\times h_t^{-1} \\
\eta_t = y_t-y_{t|t-1}
\end{align}
Using this, we can get our next period KF forecast:
\begin{align}
\xi_{t|t} = \xi_{t|t-1} + K_t\times \eta_t \\
P_{t|t} = P_{t|t-1} - K_t\cdot Cov(\xi_t,Y_t|t-1)
\end{align}
See R-script for its implementation in R.
The next graph provides a graphical representation resulting from this procedure. The blue line represents $\xi_{t|t}$ for all $t$ while the black one is the generated data.
```{r, include = TRUE}
require(ggplot2)
ggplot() +
ggtitle('Kalman Filter') +
geom_line(aes(x=c(1:length(yt)), y=yt, colour = "data")) +
geom_line(aes(x=c(1:length(yt)), y=f.xi, colour = "Kalman Filter")) +
scale_color_manual(values = c("Kalman Filter"="blue", data="black"))
```
To check the validity of our Kalman Filter, we can plot the actual measurement noise (which we randomly generated) with the KF measurement noise (which is retrieved by subtracting our Kalman forecast to the the generated data). As we can see on the next plot, the two match quite closely, indicating that our Kalman Filter was able to filter out the measurement noise quite succesfully.
```{r, include = T}
f.noise <- yt - f.xi
ggplot() +
ggtitle("Accuracy") +
geom_line(aes(x=c(1:length(yt)), y=f.noise, colour = "KF Measurement Noise")) +
geom_line(aes(x=c(1:length(yt)), y=noise, colour = "Actual Measurement Noise")) +
scale_color_manual(values = c("KF Measurement Noise"="blue", "Actual Measurement Noise"="black"))
```
Another check is to compare the actual state noise with the ones implied by the Kalman Filter. In general, this is not something that can be done as the process that generates the data is not known with certainty.
```{r}
state_noise_KF <- arima_yt - f.xi
ggplot() +
geom_line(aes(x=c(1:n_periods), y=f.xi)) +
geom_line(aes(x=c(1:n_periods), y=yt))
```
# Conclusion
This document provides a purposefully very simple application of a Kalman Filter. My wish was to be able to focus on the key ideas behind it. More complex version of this problem would for example be to consider a multi-dimensional state-space model with unknown parameters that would require to be estimated using a Maximum Likelihood estimation. I would, however, argue that considering such settings would not have added much value given that the aim of this document was to understand what is a Kalman Filter and what it does. I hope it is clearer now.
If you notice any typos, errors, or omissions, feel free to send a mail to this address: brendan.berthold@gmail.com.
# References
- Hamilton
- Mark Watson's courses in Gerzensee
ggplot() +
geom_line(aes(x=c(1:n_periods), y=f.xi)) +
geom_line(aes(x=c(1:n_periods), y=yt-state_noise))
ggplot() +
geom_line(aes(x=c(1:n_periods), y=f.xi)) +
geom_line(aes(x=c(1:n_periods), y=arima_yt))
ggplot() +
geom_line(aes(x=c(1:n_periods), y=f.xi)) +
geom_line(aes(x=c(1:n_periods), y=arima_yt-state_noise))
ggplot() +
geom_line(aes(x=c(1:n_periods), y=f.xi)) +
geom_line(aes(x=c(1:n_periods), y=arima_yt-state_noise))
ggplot() +
geom_line(aes(x=c(1:n_periods), y=f.xi)) +
geom_line(aes(x=c(1:n_periods), y=arima_yt))
ggplot() +
geom_line(aes(x=c(1:n_periods), y=f.xi), colour = "blue") +
geom_line(aes(x=c(1:n_periods), y=arima_yt), colour = "black")
ggplot() +
ggtitle("Accuracy") +
geom_line(aes(x=c(1:length(yt)), y=f.noise, colour = "KF Measurement Noise")) +
geom_line(aes(x=c(1:length(yt)), y=noise, colour = "Actual Measurement Noise")) +
scale_color_manual(values = c("KF Measurement Noise"="blue", "Actual Measurement Noise"="black"))
ggplot() +
geom_line(aes(x=c(1:n_periods), y=f.xi), colour = "blue") +
geom_line(aes(x=c(1:n_periods), y=arima_yt), colour = "black")
theme_set(theme(legend.position = "bottom"))
theme_set(theme_bw() + theme(legend.position = "bottom"))
state_noise_KF <- arima_yt - state_noise - f.xi
ggplot() +
geom_line(aes(x=c(1:n_periods), y=state_noise_KF), colour = "blue") +
geom_line(aes(x=c(1:n_periods), y=state_noise), colour = "black")
---
title: 'Intuition, theory, and a simple application of the Kalman Filter'
author: "Brendan Berthold"
date: "4/8/2020"
output:
html_document: default
editor_options:
chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, include = FALSE)
```
# Introduction
This document provides an intuitive and simple introduction to Kalman Filtering.
The document is organised as follows: the first section motivates the usage of a Kalman Filter in an economic context and reviews the related theory. In particular, it focuses on the key results and ideas behind it--as we shall see, the Kalman Filter's key idea is in reality surprisingly easy to understand. The second section applies the Kalman Filter algorithm to an extremely simple (yet not trivial) state-space model characterised by an AR(1) process as the state equation. The idea behind this is to simplify as much as possible in order to focus on the key elements that will then reveal useful to understand more complex problems.
This document is mostly based on lecture Notes from Prof. Mark Watson (Princeton University) given as part of the program for beginning doctoral students in Economics in Gerzensee taught in March 2020. The lecture notes are themselves heavily based on Hamilton's seminal book.
# The Kalman Filter for Economics : Motivation, theory, and intuition
## Motivation
We typically observe economic data over time. These data, however, are likely to be noisy. The Kalman Filter is based on the assumption that there exists some sort of underlying process (a state equation) that drives the observations but which we cannot observe because of noise and surprise shocks. The Kalman Filter is a method that allows to use the observed data in order to learn about the unobservable state variable.
The Kalman Filter was first used in physics but is now widely popular in economics.
## The theory
**Notation**
The general form of the Kalman filter as presented in Hamilton Chapter 13 (add ref) is given by a "measurement equation":
\begin{align}
y_t = A'x_t + H'\xi_t + w_t \label{eq1}
\end{align}
With $E(w_tw_t')=Q$
And a transition (or state) equation:
\begin{align}
\xi_t = F\xi_{t-1} + v_t \label{eq2}
\end{align}
With $E(v_tv_t')=R$.
In words:
- $y_t$ is the vector of observed variables (i.e. the data)
- $x_t$ is a vector of deterministic components (we won't spend time on it in this document)
- $\xi_t$ is the unobserved "state" variables
- $w_t$ and $v_t$ are unobserved, mutually and serially uncorrelated noise variables (which usually follow a Normal Distribution)
- $A, H, R, F,$ and $Q$ are non-random "system" variables matrices that may depend on unknown parameters (some of them can be retrieved using Maximum Likelihood estimations)
The general system defined by (\ref{eq1}) and (\ref{eq2}) is flexible and can accomodate a variety of representation. For instance, a standard AR(p) process fits into the general notation in the following way:
Let $y_t \sim AR(p)$, that is:
\begin{align*}
y_t = \phi_1 y_{t-1} + \phi_2y_{t-2} + ... + \phi_py_{t-p} + \epsilon_t
\end{align*}
This process can be represented as a "state-space" model in the following way:
\begin{align*}
&\xi_t = \begin{bmatrix}
y_t \\ y_{t-1} \\ \vdots \\ y_{t-p+1}
\end{bmatrix} \\
&F= \begin{bmatrix}
\phi_1 & \phi_2 & \ldots & \phi_{p-1} & \phi_p \\
1 & 0 & \ldots & 0 & 0 \\
0 & 1 &&& 0 \\
\vdots & & \ddots & & \vdots \\
0 &&&1&0
\end{bmatrix} \\
&v_t = \begin{bmatrix}
\epsilon_t \\ 0 \\ \vdots \\ 0
\end{bmatrix}
\end{align*}
And $w_t=0, A=0,$ and $H'=\begin{bmatrix}1 & 0 & \ldots & 0\end{bmatrix}$
### Procedure and idea of the Kalman Filter
Notation:
- $y_{1:t} = \left\{y_i\right\}^t_{i=1}$
- $\xi_{t|k} = E(\xi_t|y_{1:k})$
- $P_{t|k}=Var(\xi_{t}|y_{1:k})$
In words, the Kalman filter is a recursive algorithm to construct $\xi_{t|t}$ and $P_{t|t}$ from known values in $t$, that is $y_t,x_t, \xi_{t-1|t-1}, P_{t-1|t-1}$.
To derive the filter, we assume that both $w_t$ and $v_t$ follow iid Gaussian process, that is:
\begin{align*}
\begin{bmatrix}
w_t \\ v_t
\end{bmatrix} \sim N \left(\begin{bmatrix}0\\0\end{bmatrix}, \begin{bmatrix}R &0\\0&Q\end{bmatrix}\right)
\end{align*}
This notably implies that both $y_t$ and $\xi_t$ follow a _joint_ Normal distribution. Since errors are Gaussian, the best estimator (in the sense that it minimises the mean squared error) is given by the conditional expectation.
To find the conditional expectation of $\xi_t$ and $y_t$ (that is $\xi_{t|t}$ and $y_{t|t}$), we can use the following theorem of the conditional distribution of a multivariate normal:
Suppose that:
\begin{align*}
\begin{bmatrix}
z_1 \\ z_2
\end{bmatrix} \sim N \left(\begin{bmatrix}\mu_1\\\mu_2\end{bmatrix}, \begin{bmatrix}\Sigma_{11} &\Sigma_{12}\\\Sigma_{21}&\Sigma_{22}\end{bmatrix}\right)
\end{align*}
Then:
\begin{align*}
E(z_1|z_2) = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(z_2 - \mu_2) \\
Var(z_1|z_2) = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
\end{align*}
The application of this theorem is the **key idea** of the Kalman Filter.
Defining $z_1 = \xi_t$ and $z_2 = y_t$, and recognizing that $\xi_t$ and $y_t$ are jointly Normal conditional on past values, we can write the following:
\begin{align*}
\begin{bmatrix}
\xi_t \\ y_t
\end{bmatrix} \Bigg | y_{1:t-1}\sim \mathcal{N}\left(\begin{bmatrix}
\xi_{t|t-1} \\ y_{t|t-1}
\end{bmatrix}, \begin{bmatrix}
P_{t|t-1} & \Sigma_{\xi,y|t-1} \\\Sigma_{\xi,y|t-1} & \Sigma_{yy|t-1}
\end{bmatrix}\right)
\end{align*}
Using the formula of the conditional normal:
\begin{align*}
\xi_{t|t}= \xi_{t|t-1} + \Sigma_{\xi,y|t-1}\Sigma_{yy|t-1}^{-1}(y_t-y_{t|t-1}) \\
P_{t|t} = P_{t|t-1} - \Sigma_{\xi,y|t-1}\Sigma_{yy|t-1}^{-1}\Sigma_{\xi,y|t-1}
\end{align*}
Assuming $\xi_{t-1|t-1}$ and $P_{t-1|t-1}$ are known:
\begin{align}
&\xi_{t|t-1} = F\xi_{t-1|t-1} \\
&y_{t|t-1} = A'x_t + H'\xi_{t|t-1} \\
&P_{t|t-1} = FP_{t-1|t-1}F' + Q \\
&\Sigma_{yy|t-1} = H'P_{t|t-1}H + R \equiv h_t \\
&\Sigma_{\xi,y|t-1}\Sigma_{yy|t-1}^{-1} = P_{t|t-1}H h_t^{-1} \equiv K_t \\
&\eta_t = y_t-y_{t|t-1}
\end{align}
Applying the theorem, we get:
\begin{align*}
&\xi_{t|t} = \xi_{t|t-1} + K_t \eta_t\\
&P_{t|t} = P_{t|t-1} - K_t H'P_{t|t-1}
\end{align*}
If the process is covariance stationary, we can initialize it by assuming $\xi_{0|0}=0$ and using a reasonable estimate of $P_{0|0}$. We can then retrieve $x_{t|t}$, and $P_{t|t}$ for all $t>0$ applying this procedure recursively.
## Application
To better understand the algorithm let us consider the following (uni-dimensional) simple example.
```{r}
setwd("~/Dropbox/My Computer/GitHub/KalmanFilter/R-script")
source("ASimpleKF.R")
```
The state space model is of the form:
\begin{align*}
&y_t = \phi \xi_t + w_t \\
&\xi_t = \xi_{t-1} + v_t
\end{align*}
For simplicity, we assume that the transition equation is known with certainty and that $\phi=$ `r phi`. Moreover, we assume that $w_t$ and $v_t$ are i.i.d and independent of one another with $\sigma^2_w=$ `r sigma_sq_w` and $\sigma^2_v=$ `r sigma_sq_v`.
Where the generated $y_t$ looks as follows:
```{r, include = T}
ggplot() +
ggtitle("Randomly Generated Data") +
geom_line(aes(x=1:n_periods, y=yt), colour = "black")
```
The last step is to initalize the loop. We assume $\xi_{0|0}=0$ because the process is covariance stationary, and $P_{0|0}=1$ arbitrarily.
## The algorithm
Since $\xi_{0|0}$ and $P_{0|0}$ are known, we can recursively compute $\xi_{t|t}$ and $P_{t|t}$ for $t>0$ using the Kalman Filter algorithm:
\begin{align}
\xi_{t|t-1}= \phi \xi_{t-1|t-1} \\
y_{t|t-1} = \xi_{t|t-1} \\
P_{t|t-1} = \phi^2P_{t-1|t-1} + \sigma_v^2 \\
h_t \equiv Var(y_t|t-1)  = P_{t|t-1} + \sigma^2_w \\
K_t = Cov(\xi_t,Y_t|t-1)\times h_t = P_{t|t-1}\times h_t^{-1} \\
\eta_t = y_t-y_{t|t-1}
\end{align}
Using this, we can get our next period KF forecast:
\begin{align}
\xi_{t|t} = \xi_{t|t-1} + K_t\times \eta_t \\
P_{t|t} = P_{t|t-1} - K_t\cdot Cov(\xi_t,Y_t|t-1)
\end{align}
See R-script for its implementation in R.
The next graph provides a graphical representation resulting from this procedure. The blue line represents $\xi_{t|t}$ for all $t$ while the black one is the generated data.
```{r, include = TRUE}
require(ggplot2)
ggplot() +
ggtitle('Kalman Filter') +
geom_line(aes(x=c(1:length(yt)), y=yt, colour = "data")) +
geom_line(aes(x=c(1:length(yt)), y=f.xi, colour = "Kalman Filter")) +
scale_color_manual(values = c("Kalman Filter"="blue", data="black"))
```
To check the validity of our Kalman Filter, we can plot the actual measurement noise (which we randomly generated) with the KF measurement noise (which is retrieved by subtracting our Kalman forecast to the the generated data). As we can see on the next plot, the two match quite closely, indicating that our Kalman Filter was able to filter out the measurement noise quite succesfully.
```{r, include = T}
f.noise <- yt - f.xi
ggplot() +
ggtitle("Accuracy") +
geom_line(aes(x=c(1:length(yt)), y=f.noise, colour = "KF Measurement Noise")) +
geom_line(aes(x=c(1:length(yt)), y=noise, colour = "Actual Measurement Noise")) +
scale_color_manual(values = c("KF Measurement Noise"="blue", "Actual Measurement Noise"="black"))
```
Another check is to compare the actual state noise with the ones implied by the Kalman Filter. In general, this is not something that can be done as the process that generates the data is not known with certainty. Here, as we generated the data ourselves, it is possible. State noise implied by the Kalman Filter is equal to the actual AR(1) process minus our Kalman Filter forecasts.
```{r, include = T}
state_noise_KF <- arima_yt - state_noise - f.xi
ggplot() +
geom_line(aes(x=c(1:n_periods), y=state_noise_KF), colour = "blue") +
geom_line(aes(x=c(1:n_periods), y=state_noise), colour = "black")
```
# Conclusion
This document provides a purposefully very simple application of a Kalman Filter. My wish was to be able to focus on the key ideas behind it. More complex version of this problem would for example be to consider a multi-dimensional state-space model with unknown parameters that would require to be estimated using a Maximum Likelihood estimation. I would, however, argue that considering such settings would not have added much value given that the aim of this document was to understand what is a Kalman Filter and what it does. I hope it is clearer now.
If you notice any typos, errors, or omissions, feel free to send a mail to this address: brendan.berthold@gmail.com.
# References
- Hamilton
- Mark Watson's courses in Gerzensee
state_noise_KF <- -arima_yt + state_noise + f.xi
ggplot() +
geom_line(aes(x=c(1:n_periods), y=state_noise_KF), colour = "blue") +
geom_line(aes(x=c(1:n_periods), y=state_noise), colour = "black")
state_noise_KF <- f.xi-arima_yt + state_noisef.xi
state_noise_KF <- f.xi-arima_yt + state_noise
ggplot() +
geom_line(aes(x=c(1:n_periods), y=state_noise_KF), colour = "blue") +
geom_line(aes(x=c(1:n_periods), y=state_noise), colour = "black")
ggplot() +
geom_line(aes(x=c(1:n_periods), y=arima_yt), colour = "blue") +
geom_line(aes(x=c(1:n_periods), y=f.xi), colour = "black")
state_noise_KF <- f.xi-(arima_yt - state_noise)
ggplot() +
geom_line(aes(x=c(1:n_periods), y=state_noise_KF), colour = "blue") +
geom_line(aes(x=c(1:n_periods), y=state_noise), colour = "black")
state_noise_KF <- f.xi-(arima_yt - state_noise)
ggplot() +
geom_line(aes(x=c(1:n_periods), y=state_noise_KF), colour = "blue") +
geom_line(aes(x=c(1:n_periods), y=state_noise), colour = "black")
test[1] <- f.xi[1]
test <- 0
test[1] <- f.xi[1]
for(i in 2:length(f.xi)){
test[i] <- phi*f.xi[i-1]
}
ggplot() +
geom_line(aes(x=c(1:n_periods), y=test), colour = "blue") +
geom_line(aes(x=c(1:n_periods), y=state_noise), colour = "black")
ggplot() +
geom_line(aes(x=c(1:n_periods), y=f.xi-test), colour = "blue") +
geom_line(aes(x=c(1:n_periods), y=state_noise), colour = "black")
state_noise_KF <- f.xi-(arima_yt - state_noise)
ggplot() +
geom_line(aes(x=c(1:n_periods), y=state_noise_KF), colour = "blue") +
geom_line(aes(x=c(1:n_periods), y=state_noise), colour = "black")
ggplot() +
ggtitle("Actual vs KF Structural Shocks")
ggplot() +
ggtitle("Actual vs KF Structural Shocks") +
geom_line(aes(x=c(1:n_periods), y=state_noise_KF), colour = "KF Structural shocks") +
geom_line(aes(x=c(1:n_periods), y=state_noise), colour = "Actual Structural shocks") +
scale_color_manual(values = c("KF Structural shocks"="blue", "Actual Structural shocks"="black"))
state_noise_KF <- as.vector(f.xi-(arima_yt - state_noise))
ggplot() +
ggtitle("Actual vs KF Structural Shocks") +
geom_line(aes(x=c(1:n_periods), y=state_noise_KF), colour = "KF Structural shocks") +
geom_line(aes(x=c(1:n_periods), y=state_noise), colour = "Actual Structural shocks") +
scale_color_manual(values = c("KF Structural shocks"="blue", "Actual Structural shocks"="black"))
ggplot() +
ggtitle("Actual vs KF Structural Shocks") +
geom_line(aes(x=c(1:n_periods), y=state_noise_KF, colour = "KF Structural shocks")) +
geom_line(aes(x=c(1:n_periods), y=state_noise, colour = "Actual Structural shocks")) +
scale_color_manual(values = c("KF Structural shocks"="blue", "Actual Structural shocks"="black"))
ggplot() +
ggtitle('True process vs Kalman Filter') +
geom_line(aes(x=c(1:length(yt)), y=arima_yt, colour = "True process")) +
geom_line(aes(x=c(1:length(yt)), y=f.xi, colour = "Kalman Filter")) +
scale_color_manual(values = c("Kalman Filter"="blue", "True process"="black"))
ggplot() +
ggtitle('True process vs Kalman Filter') +
geom_line(aes(x=c(1:length(yt)), y=as.vector(arima_yt), colour = "True process")) +
geom_line(aes(x=c(1:length(yt)), y=f.xi, colour = "Kalman Filter")) +
scale_color_manual(values = c("Kalman Filter"="blue", "True process"="black"))
source("ASimpleKF.R")
setwd("~/Dropbox/My Computer/GitHub/ASimpleKalmanFilterWithR/R-script")
source("ASimpleKF.R")
# Setup
rm(list=ls())
require(tidyverse)
theme_set(theme_bw() + theme(legend.position = "bottom"))
# Parameters of the state space model
phi <- 0.8
sigma_sq_v <- 1
sigma_sq_w <- 1
write.csv()
# Generating data
n_periods <- 80
set.seed(1)
noise <- rnorm(mean = 0, sd = sigma_sq_w, n=n_periods)
state_noise <- rnorm(n=n_periods, mean = 0, sd = sigma_sq_v)
arima_yt <- arima.sim(list(order=c(1,0,0), ar=phi, rand.gen = state_noise), n=n_periods)
yt <- arima_yt + noise
yt <- as.vector(yt)
# Recursion algorithm
# notation: l. means the conditional expectation on t-1 while f. means the actual forecast
# Initialization
f.xi <- 0
f.P <- 100
l.xi <- 0
l.y <- 0
l.P <- 0
h_t <- 0
K_t <- 0
eta <- 0
# loop
for(i in c(2:length(yt))){
l.xi[i] <- phi*f.xi[i-1]
l.y[i] <- l.xi[i]
l.P[i] <- phi*phi*f.P[i-1] + sigma_sq_v
h_t[i] <- l.P[i] + sigma_sq_w
K_t[i] <- l.P[i]*(h_t[i])^(-1)
eta[i] <- yt[i]-l.y[i]
f.xi[i] <- l.xi[i] + K_t[i]*eta[i]
f.P[i] <- l.P[i] - K_t[i]*l.P[i]
}
